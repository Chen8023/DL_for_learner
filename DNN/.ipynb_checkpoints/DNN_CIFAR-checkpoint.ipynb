{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备\n",
    "首先看一下CIFAR-10数据是怎么读取的，下面的函数是根据官网示例改编的函数，其直接返回ndarray形式的X与Y。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    '''\n",
    "    CIFAR-10数据读取函数\n",
    "    '''\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fd:\n",
    "        data = pickle.load(fd, encoding='bytes')\n",
    "    return data[b'data'], np.array(data[b'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于深度学习中的大型数据，mini-batch式学习是很有必要的，并且还会频繁对数据做一些其他的操作。所以定义一个专门的数据类用于管理数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class CifarData:\n",
    "    def __init__(self, paths, batch_size=32, normalize=False, shuffle=False):\n",
    "        '''\n",
    "        paths: 文件路径\n",
    "        '''\n",
    "        self._data = list()\n",
    "        self._target = list()\n",
    "        self._n_samples = 0\n",
    "        self.n_features = 0\n",
    "\n",
    "        self._idx = 0    # mini-batch的游标\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        self._load(paths)\n",
    "\n",
    "        if shuffle:\n",
    "            self._shuffle_data()\n",
    "        if normalize:\n",
    "            self._normalize_data()\n",
    "\n",
    "        print(self._data.shape, self._target.shape)\n",
    "\n",
    "    def _load(self, paths):\n",
    "        '''\n",
    "        载入数据\n",
    "        '''\n",
    "        for path in paths:\n",
    "            data, labels = unpickle(path)\n",
    "            self._data.append(data)\n",
    "            self._target.append(labels)\n",
    "\n",
    "        # 将所有批次的数据拼接起来\n",
    "        self._data, self._target = np.vstack(\n",
    "            self._data), np.hstack(self._target)\n",
    "\n",
    "        self._n_samples, self.n_features = self._data.shape[0], self._data.shape[1]\n",
    "\n",
    "    def _shuffle_data(self):\n",
    "        '''\n",
    "        打乱数据\n",
    "        '''\n",
    "        idxs = np.random.permutation(self._n_samples)\n",
    "        self._data = self._data[idxs]\n",
    "        self._target = self._target[idxs]\n",
    "\n",
    "    def _normalize_data(self):\n",
    "        scaler = StandardScaler()\n",
    "        self._data = scaler.fit_transform(self._data)\n",
    "\n",
    "    def next_batch(self):\n",
    "        '''\n",
    "        生成mini-batch\n",
    "        '''\n",
    "        while self._idx < self._n_samples:\n",
    "            yield self._data[self._idx: (self._idx+self._batch_size)], self._target[self._idx: (self._idx+self._batch_size)]\n",
    "            self._idx += self._batch_size\n",
    "\n",
    "        self._idx = 0\n",
    "        self._shuffle_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_DIR = \"../dataset/cifar-10-batches-py/\"\n",
    "train_filenames = [os.path.join(\n",
    "    CIFAR_DIR, 'data_batch_{}'.format(i)) for i in range(1, 6)]\n",
    "test_filenames = [os.path.join(CIFAR_DIR, 'test_batch')]\n",
    "\n",
    "batch_size = 32\n",
    "train_data = CifarData(\n",
    "    train_filenames, batch_size=batch_size, normalize=True, shuffle=True)\n",
    "test_data = CifarData(test_filenames, batch_size=batch_size,\n",
    "                      normalize=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络结构设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入层单元数为(数据特征)，设计隐藏层单元数为100、50，输出单元数为10(多分类任务)\n",
    "\n",
    "unit_I = train_data.n_features    # 输入层的单元数，与特征数相等\n",
    "unit_h1 = 100    # 第一层隐藏层的单元数\n",
    "unit_h2 = 50\n",
    "unit_O = 10    # 输出层单元数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建网络\n",
    "TensorFlow自身提供了增加层数的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入必须是可由用户指定的，所以设为placeholder\n",
    "X = tf.placeholder(tf.float32, [None, unit_I])  # 数据的样本数不指定，只指定特征数\n",
    "Y = tf.placeholder(tf.int64, [None])    # 目标值为列向量，int64为了兼容\n",
    "\n",
    "# 网络结构图\n",
    "with tf.name_scope('DNN'):\n",
    "    hidden1 = tf.layers.dense(X, unit_h1, activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, unit_h2, activation=tf.nn.relu)\n",
    "    Y_pred = tf.layers.dense(hidden2, unit_O)\n",
    "\n",
    "# 评估子图\n",
    "with tf.name_scope('Eval'):\n",
    "    # 计算一维向量与onehot向量之间的损失\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=Y, logits=Y_pred)\n",
    "    predict = tf.argmax(Y_pred, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predict, Y), tf.float32))\n",
    "\n",
    "# 优化图\n",
    "with tf.name_scope('train_op'):\n",
    "    lr = 1e-3\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    epochs = 1000\n",
    "\n",
    "    batch_cnt = 0\n",
    "    for epoch in range(epochs):\n",
    "        for batch_data, batch_labels in train_data.next_batch():\n",
    "            batch_cnt += 1\n",
    "            loss_val, acc_val, _ = sess.run(\n",
    "                [loss, accuracy, train_op],\n",
    "                feed_dict={\n",
    "                    X: batch_data,\n",
    "                    Y: batch_labels})\n",
    "\n",
    "            # 每1000batch输出一次信息\n",
    "            if (batch_cnt+1) % 1000 == 0:\n",
    "                print('epoch: {}, batch_loss: {}, batch_acc: {}'.format(\n",
    "                    epoch, loss_val, acc_val))\n",
    "\n",
    "            # 每5000batch做一次验证\n",
    "            if (batch_cnt+1) % 5000 == 0:\n",
    "                all_test_acc_val = list()\n",
    "                for test_batch_data, test_batch_labels in test_data.next_batch():\n",
    "                    test_acc_val = sess.run(\n",
    "                        [accuracy],\n",
    "                        feed_dict={\n",
    "                            X: test_batch_data,\n",
    "                            Y: test_batch_labels\n",
    "                        })\n",
    "                    all_test_acc_val.append(test_acc_val)\n",
    "                test_acc = np.mean(all_test_acc_val)\n",
    "                print('epoch: {}, test_acc: {}'.format(epoch, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
