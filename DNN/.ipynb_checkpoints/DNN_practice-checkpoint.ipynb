{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备\n",
    "首先看一下CIFAR-10数据是怎么读取的，下面的函数是根据官网示例改编的函数，其直接返回ndarray形式的X与Y。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def unpickle(file):\n",
    "    '''\n",
    "    CIFAR-10数据读取函数\n",
    "    '''\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fd:\n",
    "        data = pickle.load(fd, encoding='bytes')\n",
    "    return data[b'data'], np.array(data[b'labels'])\n",
    "\n",
    "\n",
    "data, target = unpickle('../dataset/cifar-10-batches-py/data_batch_1')\n",
    "print(data.shape, target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于深度学习中的大型数据，mini-batch式学习是很有必要的，并且还会频繁对数据做一些其他的操作。所以定义一个专门的数据类用于管理数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler    # 数据标准化，也可使用BN\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self, paths, batch_size=32,normalize=True, shuffle=False):\n",
    "        '''\n",
    "        paths: 文件路径\n",
    "        '''\n",
    "        self.data = list()\n",
    "        self.target = list()\n",
    "        self.n_samples = None\n",
    "        self.n_features = None\n",
    "\n",
    "        self.idx = 0    # mini-batch的游标\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self._load(paths)\n",
    "        \n",
    "        if shuffle:\n",
    "            self._shuffle_data()\n",
    "        if normalize:\n",
    "            self._normalize_data()\n",
    "            \n",
    "        print(self.data.shape, self.target.shape)\n",
    "\n",
    "    def _load(self, paths):\n",
    "        '''\n",
    "        载入数据\n",
    "        '''\n",
    "        for path in paths:\n",
    "            cur_data, cur_target = unpickle(path)\n",
    "            self.data.append(cur_data)\n",
    "            self.target.append(cur_target)\n",
    "\n",
    "        # 将所有批次的数据拼接起来\n",
    "        self.data = np.vstack(self.data)\n",
    "        self.target = np.hstack(self.target).reshape((-1, 1))\n",
    "\n",
    "        self.n_samples, self.n_features = self.data.shape[0], self.data.shape[1]\n",
    "\n",
    "    def _shuffle_data(self):\n",
    "        '''\n",
    "        打乱数据\n",
    "        '''\n",
    "        idxs = np.random.permutation(self.n_samples)\n",
    "        self.data = self.data[idxs]\n",
    "        self.target = self.target[idxs]\n",
    "        \n",
    "    def _normalize_data(self):\n",
    "        scaler=StandardScaler()\n",
    "        self.data=scaler.fit_transform(self.data)\n",
    "\n",
    "    def next_batch(self):\n",
    "        '''\n",
    "        生成mini-batch\n",
    "        '''\n",
    "        while self.idx < self.n_samples:\n",
    "            yield (self.data[self.idx:self.idx+self.batch_size], self.target[self.idx:self.idx+self.batch_size])\n",
    "            self.idx += self.batch_size\n",
    "\n",
    "        self.idx = 0\n",
    "        self._shuffle_data()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\qq435\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\users\\qq435\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072) (10000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\qq435\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072) (10000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\qq435\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "CIFAR_DIR='../dataset/cifar-10-batches-py/'\n",
    "train_paths=[os.path.join(CIFAR_DIR, 'data_batch_{}'.format(i)) for i in range(1, 2)]\n",
    "val_paths=[os.path.join(CIFAR_DIR, 'test_batch')]\n",
    "\n",
    "batch_size=10000\n",
    "train_data = DataSet(train_paths,batch_size=batch_size, shuffle=True)\n",
    "val_data=DataSet(val_paths,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8500, 3072) (8500, 1) (1500, 3072) (1500, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler=StandardScaler()\n",
    "X_sca=scaler.fit_transform(train_data.data)\n",
    "Y=train_data.target\n",
    "\n",
    "X_train,X_val,Y_train,Y_val=train_test_split(X_sca,Y,test_size=0.15)\n",
    "\n",
    "print(X_train.shape, Y_train.shape,X_val.shape,Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络结构设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入层单元数为64(数据特征)，设计隐藏层单元数为5，输出单元数为10(多分类任务)\n",
    "\n",
    "unit_I = X_train.shape[1]    # 输入层的单元数，与特征数相等\n",
    "unit_h1 = 100    # 第一层隐藏层的单元数\n",
    "unit_h2 = 50\n",
    "unit_O = 10    # 输出层单元数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建网络\n",
    "TensorFlow自身提供了增加层数的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入必须是可由用户指定的，所以设为placeholder\n",
    "X = tf.placeholder(tf.float32, shape=[None, unit_I])    # 数据的样本数不指定，只指定特征数\n",
    "Y = tf.placeholder(tf.int32, shape=[None, 1])    # 目标值为列向量\n",
    "\n",
    "# 网络结构子图\n",
    "with tf.name_scope('DNN'):\n",
    "    a1 = tf.layers.dense(X, unit_h1, activation=tf.nn.relu)\n",
    "    a2 = tf.layers.dense(a1, unit_h2, activation=tf.nn.relu)\n",
    "    Y_pred = tf.layers.dense(a2, unit_O, activation=tf.nn.softmax)\n",
    "\n",
    "# 损失函数子图\n",
    "with tf.name_scope('Loss'):\n",
    "    # 计算一维向量与onehot向量之间的损失\n",
    "    cross_entropy = tf.losses.sparse_softmax_cross_entropy(\n",
    "        labels=Y, logits=Y_pred)\n",
    "\n",
    "# optimization子图\n",
    "lr = 0.01    # 学习率\n",
    "with tf.name_scope('Train'):\n",
    "    opt = tf.train.AdamOptimizer(lr).minimize(cross_entropy)    # Adam优化器\n",
    "\n",
    "init = tf.global_variables_initializer()    # 所有变量初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "test_log_loss: 2.038086414337158\tacc:0.4208\n"
     ]
    }
   ],
   "source": [
    "# 计算图已经构建好，开启一个tf会话，需要计算哪个值就run哪个变量即可\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    train_iter = 500\n",
    "\n",
    "    cnt=0\n",
    "    for i in range(train_iter):\n",
    "        # mini-batch\n",
    "        for X_batch,Y_batch in train_data.next_batch():\n",
    "            sess.run(opt, feed_dict={X: X_batch, Y: Y_batch})\n",
    "            cnt+=1\n",
    "            if cnt%500==0:\n",
    "                loss_train = sess.run(cross_entropy, feed_dict={X: X_batch, Y: Y_batch})\n",
    "                print('train_iter: {}\\tlog_loss:{}'.format(i, loss_train))\n",
    "\n",
    "        # batch\n",
    "#         sess.run(opt, feed_dict={X: train_data.data, Y: train_data.target})\n",
    "#         if i % 100 == 0:\n",
    "#             loss_train = sess.run(cross_entropy, feed_dict={\n",
    "#                                   X: train_data.data, Y: train_data.target})\n",
    "#             print('train_iter: {}\\tlog_loss:{}'.format(i, loss_train))\n",
    "\n",
    "    pred = sess.run(Y_pred, feed_dict={X: val_data.data, Y: val_data.target})\n",
    "    loss_val = sess.run(cross_entropy, feed_dict={X: val_data.data, Y: val_data.target})\n",
    "    acc = np.sum(np.squeeze(val_data.target) == np.argmax(pred, axis=1))/len(val_data.target)\n",
    "\n",
    "    print('test_log_loss: {}\\tacc:{}'.format(loss_val, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
