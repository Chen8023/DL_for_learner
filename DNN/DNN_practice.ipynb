{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备\n",
    "首先看一下CIFAR-10数据是怎么读取的，下面的函数是根据官网示例改编的函数，其直接返回ndarray形式的X与Y。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def unpickle(file):\n",
    "    '''\n",
    "    CIFAR-10数据读取函数\n",
    "    '''\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fd:\n",
    "        data = pickle.load(fd, encoding='bytes')\n",
    "    return data[b'data'], np.array(data[b'labels'])\n",
    "\n",
    "\n",
    "data, target = unpickle('../dataset/cifar-10-batches-py/data_batch_1')\n",
    "print(data.shape, target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于深度学习中的大型数据，mini-batch式学习是很有必要的，并且还会频繁对数据做一些其他的操作。所以定义一个专门的数据类用于管理数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler    # 数据标准化，也可使用BN\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self, paths, batch_size=32,normalize=True, shuffle=False):\n",
    "        '''\n",
    "        paths: 文件路径\n",
    "        '''\n",
    "        self.data = list()\n",
    "        self.target = list()\n",
    "        self.n_samples = None\n",
    "        self.n_features = None\n",
    "\n",
    "        self.idx = 0    # mini-batch的游标\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self._load(paths)\n",
    "        \n",
    "        if shuffle:\n",
    "            self._shuffle_data()\n",
    "        if normalize:\n",
    "            self._normalize_data()\n",
    "            \n",
    "        print(self.data.shape, self.target.shape)\n",
    "\n",
    "    def _load(self, paths):\n",
    "        '''\n",
    "        载入数据\n",
    "        '''\n",
    "        for path in paths:\n",
    "            cur_data, cur_target = unpickle(path)\n",
    "            self.data.append(cur_data)\n",
    "            self.target.append(cur_target)\n",
    "\n",
    "        # 将所有批次的数据拼接起来\n",
    "        self.data = np.vstack(self.data)\n",
    "        self.target = np.hstack(self.target).reshape((-1, 1))\n",
    "\n",
    "        self.n_samples, self.n_features = self.data.shape[0], self.data.shape[1]\n",
    "\n",
    "    def _shuffle_data(self):\n",
    "        '''\n",
    "        打乱数据\n",
    "        '''\n",
    "        idxs = np.random.permutation(self.n_samples)\n",
    "        self.data = self.data[idxs]\n",
    "        self.target = self.target[idxs]\n",
    "        \n",
    "    def _normalize_data(self):\n",
    "        scaler=StandardScaler()\n",
    "        self.data=scaler.fit_transform(self.data)\n",
    "\n",
    "    def next_batch(self):\n",
    "        '''\n",
    "        生成mini-batch\n",
    "        '''\n",
    "        while self.idx < self.n_samples:\n",
    "            yield (self.data[self.idx:self.idx+self.batch_size], self.target[self.idx:self.idx+self.batch_size])\n",
    "            self.idx += self.batch_size\n",
    "\n",
    "        self.idx = 0\n",
    "        self._shuffle_data()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\qq435\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\users\\qq435\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072) (10000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\qq435\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072) (10000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\qq435\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "CIFAR_DIR='../dataset/cifar-10-batches-py/'\n",
    "train_paths=[os.path.join(CIFAR_DIR, 'data_batch_{}'.format(i)) for i in range(1, 2)]\n",
    "val_paths=[os.path.join(CIFAR_DIR, 'test_batch')]\n",
    "\n",
    "batch_size=10000\n",
    "train_data = DataSet(train_paths,batch_size=batch_size, shuffle=True)\n",
    "val_data=DataSet(val_paths,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8500, 3072) (8500, 1) (1500, 3072) (1500, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler=StandardScaler()\n",
    "X_sca=scaler.fit_transform(train_data.data)\n",
    "Y=train_data.target\n",
    "\n",
    "X_train,X_val,Y_train,Y_val=train_test_split(X_sca,Y,test_size=0.15)\n",
    "\n",
    "print(X_train.shape, Y_train.shape,X_val.shape,Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络结构设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入层单元数为64(数据特征)，设计隐藏层单元数为5，输出单元数为10(多分类任务)\n",
    "\n",
    "unit_I = X_train.shape[1]    # 输入层的单元数，与特征数相等\n",
    "unit_h1 = 100    # 第一层隐藏层的单元数\n",
    "unit_h2 = 50\n",
    "unit_O = 10    # 输出层单元数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建网络\n",
    "TensorFlow自身提供了增加层数的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入必须是可由用户指定的，所以设为placeholder\n",
    "X = tf.placeholder(tf.float32, shape=[None, unit_I])    # 数据的样本数不指定，只指定特征数\n",
    "Y = tf.placeholder(tf.int32, shape=[None, 1])    # 目标值为列向量\n",
    "\n",
    "# 网络结构子图\n",
    "with tf.name_scope('DNN'):\n",
    "    a1 = tf.layers.dense(X, unit_h1, activation=tf.nn.relu)\n",
    "    a2 = tf.layers.dense(a1, unit_h2, activation=tf.nn.relu)\n",
    "    Y_pred = tf.layers.dense(a2, unit_O, activation=tf.nn.softmax)\n",
    "\n",
    "# 损失函数子图\n",
    "with tf.name_scope('Loss'):\n",
    "    # 计算一维向量与onehot向量之间的损失\n",
    "    cross_entropy = tf.losses.sparse_softmax_cross_entropy(\n",
    "        labels=Y, logits=Y_pred)\n",
    "\n",
    "# optimization子图\n",
    "lr = 0.01    # 学习率\n",
    "with tf.name_scope('Train'):\n",
    "    opt = tf.train.AdamOptimizer(lr).minimize(cross_entropy)    # Adam优化器\n",
    "\n",
    "init = tf.global_variables_initializer()    # 所有变量初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_iter: 0\tlog_loss:2.253267765045166\n",
      "train_iter: 1\tlog_loss:2.2464399337768555\n",
      "train_iter: 2\tlog_loss:2.2075226306915283\n",
      "train_iter: 3\tlog_loss:2.1974220275878906\n",
      "train_iter: 4\tlog_loss:2.1998605728149414\n",
      "train_iter: 5\tlog_loss:2.1781411170959473\n",
      "train_iter: 6\tlog_loss:2.1746771335601807\n",
      "train_iter: 7\tlog_loss:2.1734468936920166\n",
      "train_iter: 8\tlog_loss:2.160944938659668\n",
      "train_iter: 9\tlog_loss:2.1559672355651855\n",
      "train_iter: 10\tlog_loss:2.149648666381836\n",
      "train_iter: 11\tlog_loss:2.145118236541748\n",
      "train_iter: 12\tlog_loss:2.1520602703094482\n",
      "train_iter: 13\tlog_loss:2.1364262104034424\n",
      "train_iter: 14\tlog_loss:2.1340649127960205\n",
      "train_iter: 15\tlog_loss:2.130308151245117\n",
      "train_iter: 16\tlog_loss:2.127180576324463\n",
      "train_iter: 17\tlog_loss:2.1242282390594482\n",
      "train_iter: 18\tlog_loss:2.11977219581604\n",
      "train_iter: 19\tlog_loss:2.116872549057007\n",
      "train_iter: 20\tlog_loss:2.117586851119995\n",
      "train_iter: 21\tlog_loss:2.1144206523895264\n",
      "train_iter: 22\tlog_loss:2.111889362335205\n",
      "train_iter: 23\tlog_loss:2.1084601879119873\n",
      "train_iter: 24\tlog_loss:2.1056013107299805\n",
      "train_iter: 25\tlog_loss:2.1031360626220703\n",
      "train_iter: 26\tlog_loss:2.099149703979492\n",
      "train_iter: 27\tlog_loss:2.1006991863250732\n",
      "train_iter: 28\tlog_loss:2.0936264991760254\n",
      "train_iter: 29\tlog_loss:2.094119071960449\n",
      "train_iter: 30\tlog_loss:2.090272903442383\n",
      "train_iter: 31\tlog_loss:2.089416027069092\n",
      "train_iter: 32\tlog_loss:2.085414409637451\n",
      "train_iter: 33\tlog_loss:2.0845913887023926\n",
      "train_iter: 34\tlog_loss:2.0877182483673096\n",
      "train_iter: 35\tlog_loss:2.082002639770508\n",
      "train_iter: 36\tlog_loss:2.0795655250549316\n",
      "train_iter: 37\tlog_loss:2.081698179244995\n",
      "train_iter: 38\tlog_loss:2.07991886138916\n",
      "train_iter: 39\tlog_loss:2.0759315490722656\n",
      "train_iter: 40\tlog_loss:2.071586847305298\n",
      "train_iter: 41\tlog_loss:2.0726053714752197\n",
      "train_iter: 42\tlog_loss:2.0691702365875244\n",
      "train_iter: 43\tlog_loss:2.0652191638946533\n",
      "train_iter: 44\tlog_loss:2.067443370819092\n",
      "train_iter: 45\tlog_loss:2.066875696182251\n",
      "train_iter: 46\tlog_loss:2.064269542694092\n",
      "train_iter: 47\tlog_loss:2.0621395111083984\n",
      "train_iter: 48\tlog_loss:2.065091371536255\n",
      "train_iter: 49\tlog_loss:2.0611231327056885\n",
      "train_iter: 50\tlog_loss:2.0592522621154785\n",
      "train_iter: 51\tlog_loss:2.05633807182312\n",
      "train_iter: 52\tlog_loss:2.053595781326294\n",
      "train_iter: 53\tlog_loss:2.0533370971679688\n",
      "train_iter: 54\tlog_loss:2.0523242950439453\n",
      "train_iter: 55\tlog_loss:2.04947566986084\n",
      "train_iter: 56\tlog_loss:2.0509207248687744\n",
      "train_iter: 57\tlog_loss:2.047480821609497\n",
      "train_iter: 58\tlog_loss:2.046780824661255\n",
      "train_iter: 59\tlog_loss:2.047109842300415\n",
      "train_iter: 60\tlog_loss:2.045675754547119\n",
      "train_iter: 61\tlog_loss:2.043729543685913\n",
      "train_iter: 62\tlog_loss:2.0454258918762207\n",
      "train_iter: 63\tlog_loss:2.04133939743042\n",
      "train_iter: 64\tlog_loss:2.039010524749756\n",
      "train_iter: 65\tlog_loss:2.0453195571899414\n",
      "train_iter: 66\tlog_loss:2.045380115509033\n",
      "train_iter: 67\tlog_loss:2.042689085006714\n",
      "train_iter: 68\tlog_loss:2.0362446308135986\n",
      "train_iter: 69\tlog_loss:2.0356991291046143\n",
      "train_iter: 70\tlog_loss:2.040158271789551\n",
      "train_iter: 71\tlog_loss:2.040038585662842\n",
      "train_iter: 72\tlog_loss:2.0382230281829834\n",
      "train_iter: 73\tlog_loss:2.0325796604156494\n",
      "train_iter: 74\tlog_loss:2.0348737239837646\n",
      "train_iter: 75\tlog_loss:2.0328941345214844\n",
      "train_iter: 76\tlog_loss:2.031146764755249\n",
      "train_iter: 77\tlog_loss:2.0322206020355225\n",
      "train_iter: 78\tlog_loss:2.0252022743225098\n",
      "train_iter: 79\tlog_loss:2.025608539581299\n",
      "train_iter: 80\tlog_loss:2.0244481563568115\n",
      "train_iter: 81\tlog_loss:2.02243709564209\n",
      "train_iter: 82\tlog_loss:2.0178136825561523\n",
      "train_iter: 83\tlog_loss:2.017427444458008\n",
      "train_iter: 84\tlog_loss:2.0166211128234863\n",
      "train_iter: 85\tlog_loss:2.0196125507354736\n",
      "train_iter: 86\tlog_loss:2.01896333694458\n",
      "train_iter: 87\tlog_loss:2.0170836448669434\n",
      "train_iter: 88\tlog_loss:2.015155792236328\n",
      "train_iter: 89\tlog_loss:2.01175856590271\n",
      "train_iter: 90\tlog_loss:2.009798765182495\n",
      "train_iter: 91\tlog_loss:2.0080831050872803\n",
      "train_iter: 92\tlog_loss:2.0102617740631104\n",
      "train_iter: 93\tlog_loss:2.010627031326294\n",
      "train_iter: 94\tlog_loss:2.006080389022827\n",
      "train_iter: 95\tlog_loss:2.009051561355591\n",
      "train_iter: 96\tlog_loss:2.005159616470337\n",
      "train_iter: 97\tlog_loss:2.0061304569244385\n",
      "train_iter: 98\tlog_loss:2.0023436546325684\n",
      "train_iter: 99\tlog_loss:2.003814697265625\n",
      "train_iter: 100\tlog_loss:1.9990307092666626\n",
      "train_iter: 101\tlog_loss:2.000474691390991\n",
      "train_iter: 102\tlog_loss:1.9969626665115356\n",
      "train_iter: 103\tlog_loss:1.9964627027511597\n",
      "train_iter: 104\tlog_loss:1.9964978694915771\n",
      "train_iter: 105\tlog_loss:1.991929292678833\n",
      "train_iter: 106\tlog_loss:1.993134617805481\n",
      "train_iter: 107\tlog_loss:1.9909974336624146\n",
      "train_iter: 108\tlog_loss:1.989563226699829\n",
      "train_iter: 109\tlog_loss:1.9885538816452026\n",
      "train_iter: 110\tlog_loss:1.991194486618042\n",
      "train_iter: 111\tlog_loss:1.9838261604309082\n",
      "train_iter: 112\tlog_loss:1.990872859954834\n",
      "train_iter: 113\tlog_loss:1.988751769065857\n",
      "train_iter: 114\tlog_loss:1.992889404296875\n",
      "train_iter: 115\tlog_loss:1.9835326671600342\n",
      "train_iter: 116\tlog_loss:1.9863905906677246\n",
      "train_iter: 117\tlog_loss:1.9806088209152222\n",
      "train_iter: 118\tlog_loss:1.979742169380188\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-96cc23fe0ee4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# mini-batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0mcnt\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcnt\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qq435\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qq435\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1102\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32mc:\\users\\qq435\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \"\"\"\n\u001b[1;32m--> 501\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 计算图已经构建好，开启一个tf会话，需要计算哪个值就run哪个变量即可\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    train_iter = 500\n",
    "\n",
    "    cnt=0\n",
    "    for i in range(train_iter):\n",
    "        # mini-batch\n",
    "        for X_batch,Y_batch in train_data.next_batch():\n",
    "            sess.run(opt, feed_dict={X: X_batch, Y: Y_batch})\n",
    "            cnt+=1\n",
    "            if cnt%1==0:\n",
    "                loss_train = sess.run(cross_entropy, feed_dict={X: X_batch, Y: Y_batch})\n",
    "                print('train_iter: {}\\tlog_loss:{}'.format(i, loss_train))\n",
    "\n",
    "        # batch\n",
    "#         sess.run(opt, feed_dict={X: train_data.data, Y: train_data.target})\n",
    "#         if i % 100 == 0:\n",
    "#             loss_train = sess.run(cross_entropy, feed_dict={\n",
    "#                                   X: train_data.data, Y: train_data.target})\n",
    "#             print('train_iter: {}\\tlog_loss:{}'.format(i, loss_train))\n",
    "\n",
    "    pred = sess.run(Y_pred, feed_dict={X: val_data.data, Y: val_data.target})\n",
    "    loss_val = sess.run(cross_entropy, feed_dict={X: val_data.data, Y: val_data.target})\n",
    "    acc = np.sum(np.squeeze(val_data.target) == np.argmax(pred, axis=1))/len(val_data.target)\n",
    "\n",
    "    print('test_log_loss: {}\\tacc:{}'.format(loss_val, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
