{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# tf.logging.set_verbosity(tf.logging.INFO)    # 代替print()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        emb_size=64,\n",
    "        t_size=50,\n",
    "        lstm_size=[32, 32],\n",
    "        lstm_layers=2,\n",
    "        fc_size=32,\n",
    "        dropout_rate=0.2,\n",
    "        batch_size=64,\n",
    "        grad_thresh=1.0,    # 梯度阈值\n",
    "        lr=0.001,\n",
    "        cnt_thresh=10,    # 词的频率阈值\n",
    "    )\n",
    "\n",
    "\n",
    "params = get_default_params()\n",
    "\n",
    "# 分词后的文件\n",
    "seg_train_file = 'cnews.seg_train.txt'\n",
    "seg_val_file = 'cnews.seg_val.txt'\n",
    "seg_test_file = 'cnews.seg_test.txt'\n",
    "# 词表\n",
    "vocal_table = 'cnews.vocal.txt'\n",
    "# 类别表\n",
    "cat_file = 'cnews.cat.txt'\n",
    "# 输出路径\n",
    "out_path = './out'\n",
    "\n",
    "if not os.path.exists(out_path):\n",
    "    os.mkdir(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据封装\n",
    "对于非结构化数据，数据部分的处理才是最麻烦的。为了便于管理，定义数据类是很有必要的。这里分为两块，第一块是用于编码的数据类，第二块是用于神经网络的数据类，类似于之前的```CifarData```，主要API为```next_batch```。\n",
    "\n",
    "第一块数据类，用于encoding："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocal:\n",
    "    '''\n",
    "    词典类，用于将word转成id\n",
    "    '''\n",
    "\n",
    "    def __init__(self, voc_file, cnt_thresh):\n",
    "        '''\n",
    "        voc_file: 词典文件\n",
    "        cnt_thresh: 频率阈值\n",
    "        '''\n",
    "        self._word2id = dict()\n",
    "        self._unk = 0    # 未知单词的编码\n",
    "        self._cnt_thresh = cnt_thresh\n",
    "        self._load_table(voc_file)\n",
    "        \n",
    "    @property\n",
    "    def unk(self):\n",
    "        '''\n",
    "        位置单词编码，用于外界padding时调用\n",
    "        '''\n",
    "        return self._unk\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self._word2id)\n",
    "\n",
    "    def _load_table(self, filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as fd:\n",
    "            data = fd.readlines()\n",
    "\n",
    "        for line in data:\n",
    "            idx, word, cnt = line.strip().split('\\t')\n",
    "            cnt = int(cnt)\n",
    "\n",
    "            if cnt < self._cnt_thresh:\n",
    "                continue\n",
    "\n",
    "            self._word2id[word] = int(idx)\n",
    "\n",
    "    def word2id(self, word):\n",
    "        return self._word2id.get(word, self._unk)\n",
    "\n",
    "    def sentence2id(self, s):\n",
    "        s_id = [self._word2id.get(word, self._unk)\n",
    "                for word in s.split()]\n",
    "        return s_id\n",
    "\n",
    "# test\n",
    "# voc_cls = Vocal(vocal_table, params.cnt_thresh)\n",
    "# print(voc_cls.size)\n",
    "# print(voc_cls.word2id('的'))    # 该句应该返回2\n",
    "# print(voc_cls.sentence2id('的 在 了 是'))    # 该句应该返回[2,4,6,7]\n",
    "\n",
    "\n",
    "class CatDict:\n",
    "    def __init__(self, cat_file):\n",
    "        '''\n",
    "        cat_file: 类别文件\n",
    "        '''\n",
    "        self._cat2id = dict()\n",
    "        self._load_table(cat_file)\n",
    "\n",
    "    def _load_table(self, filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as fd:\n",
    "            data = fd.readlines()\n",
    "\n",
    "        for line in data:\n",
    "            idx, cat, _ = line.split('\\t')\n",
    "            self._cat2id[cat] = int(idx)\n",
    "\n",
    "    def cat2id(self, cat):\n",
    "        if cat not in self._cat2id:\n",
    "            raise Exception('{} is not in cat'.format(cat))\n",
    "        else:\n",
    "            return self._cat2id[cat]\n",
    "\n",
    "# test\n",
    "# cat_dict=CatDict(cat_file)\n",
    "# print(cat_dict.cat2id('时尚'))    # 此句应该输出1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二块数据类，用于产生格式化的batch数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 50) (50000,)\n",
      "(5000, 50) (5000,)\n",
      "(10000, 50) (10000,)\n"
     ]
    }
   ],
   "source": [
    "class TextData:\n",
    "    def __init__(self, filename, vocal, cat_dict, t_size=30, batch_size=32, shuffle=True):\n",
    "        '''\n",
    "        filename: 导入的源文件，以该源文件生成格式化数据\n",
    "        vocal: 词典类\n",
    "        cat_dict: 类别类\n",
    "        t_size: \n",
    "        '''\n",
    "        self._data = list()\n",
    "        self._target = list()\n",
    "        self._n_samples = 0\n",
    "\n",
    "        self._idx = 0  # mini-batch的游标\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        self._vocal = vocal\n",
    "        self._cat_dict = cat_dict\n",
    "        self._t_size = t_size\n",
    "\n",
    "        self._load_data(filename)\n",
    "\n",
    "        if shuffle:\n",
    "            self._shuffle_data()\n",
    "            \n",
    "        print(self._data.shape, self._target.shape)\n",
    "\n",
    "    def _load_data(self, filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as fd:\n",
    "            text = fd.readlines()\n",
    "\n",
    "        for line in text:\n",
    "            label, content = line.strip().split('\\t')\n",
    "            y = self._cat_dict.cat2id(label)\n",
    "            x = self._vocal.sentence2id(content)\n",
    "\n",
    "            x = x[:self._t_size]\n",
    "            n_pad = self._t_size-len(x)    # 需要填充的位数\n",
    "            x = x+[self._vocal.unk for _ in range(n_pad)]\n",
    "\n",
    "            self._data.append(x)\n",
    "            self._target.append(y)\n",
    "\n",
    "        self._data = np.array(self._data)\n",
    "        self._target = np.array(self._target)\n",
    "        self._n_samples = self._data.shape[0]\n",
    "\n",
    "    def _shuffle_data(self):\n",
    "        '''\n",
    "        打乱数据\n",
    "        '''\n",
    "        idxs = np.random.permutation(self._n_samples)\n",
    "        self._data = self._data[idxs]\n",
    "        self._target = self._target[idxs]\n",
    "\n",
    "    def next_batch(self):\n",
    "        '''\n",
    "        生成mini-batch\n",
    "        '''\n",
    "        while self._idx + self._batch_size < self._n_samples:\n",
    "            yield self._data[self._idx: (self._idx + self._batch_size)], self._target[self._idx: (self._idx + self._batch_size)]\n",
    "            self._idx += self._batch_size\n",
    "\n",
    "        self._idx = 0\n",
    "        self._shuffle_data()\n",
    "\n",
    "\n",
    "# test\n",
    "voc_cls = Vocal(vocal_table, params.cnt_thresh)    # 词典类\n",
    "cat_dict = CatDict(cat_file)    # 类别类\n",
    "train_data = TextData(seg_train_file, voc_cls, cat_dict, params.t_size,batch_size=params.batch_size)\n",
    "val_data = TextData(seg_val_file, voc_cls, cat_dict, params.t_size,batch_size=params.batch_size)\n",
    "test_data = TextData(seg_test_file, voc_cls, cat_dict, params.t_size,batch_size=params.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# voc_cls = Vocal(vocal_table, params.cnt_thresh)    # 词典类\n",
    "vocal_size = voc_cls.size\n",
    "\n",
    "unit_O = 10    # 输出单元数，类别数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_lstm_layer(inputs, unit_I, unit_O, t_size=5, batch_size=32, init=tf.random_uniform_initializer(-1, 1)):\n",
    "    '''\n",
    "    生成一层LSTM\n",
    "    inputs: 序列数据，维度为(n_samples,t_size,n_features)，也可为(n_samples,t_size)\n",
    "    '''\n",
    "    def gen_params(unit_I, unit_O):\n",
    "        '''\n",
    "        生成权重与偏置参数\n",
    "        '''\n",
    "        w_x = tf.get_variable('w_x',shape=[unit_I, unit_O])\n",
    "        w_h = tf.get_variable('w_h',shape=[unit_O, unit_O])\n",
    "        b = tf.get_variable('bias',shape=[1, unit_O],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        return w_x, w_h, b\n",
    "\n",
    "    with tf.variable_scope('LSTM_layer', initializer=init):\n",
    "        with tf.variable_scope('i'):\n",
    "            w_ix, w_ih, b_i = gen_params(unit_I, unit_O)\n",
    "        with tf.variable_scope('f'):\n",
    "            w_fx, w_fh, b_f = gen_params(unit_I, unit_O)\n",
    "        with tf.variable_scope('g'):\n",
    "            w_gx, w_gh, b_g = gen_params(unit_I, unit_O)\n",
    "        with tf.variable_scope('o'):\n",
    "            w_ox, w_oh, b_o = gen_params(unit_I, unit_O)\n",
    "\n",
    "        # 初始的c与h，零初始化\n",
    "        c = tf.Variable(tf.zeros([batch_size, unit_O]), trainable=False)\n",
    "        h = tf.Variable(tf.zeros([batch_size, unit_O]), trainable=False)\n",
    "\n",
    "        for t in range(t_size):\n",
    "            input_t = inputs[:, t, :]    # 提取时间维度\n",
    "            input_t = tf.reshape(input_t, [batch_size, unit_I])\n",
    "\n",
    "            f = tf.sigmoid(tf.matmul(input_t, w_fx)+tf.matmul(h, w_fh)+b_f)\n",
    "            i = tf.sigmoid(tf.matmul(input_t, w_ix)+tf.matmul(h, w_ih)+b_i)\n",
    "            g = tf.tanh(tf.matmul(input_t, w_gx)+tf.matmul(h, w_gh)+b_g)\n",
    "            o = tf.sigmoid(tf.matmul(input_t, w_ox)+tf.matmul(h, w_oh)+b_o)\n",
    "\n",
    "            c = c*f+g*i\n",
    "            h = o*tf.tanh(c)\n",
    "            \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-7-0ab5357dd8f7>:44: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-0ab5357dd8f7>:45: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.int32, [None, params.t_size])\n",
    "Y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)    # 训练标识位\n",
    "\n",
    "# 自动计数\n",
    "global_step = tf.Variable(tf.zeros([], tf.int32),\n",
    "                          name='global_step', trainable=False)\n",
    "\n",
    "with tf.variable_scope('emb', initializer=tf.random_uniform_initializer(-1.0, 1.0)):\n",
    "    emb_lookup = tf.get_variable('embedding', [vocal_size, params.emb_size],\n",
    "                                 dtype=tf.float32)\n",
    "    emb = tf.nn.embedding_lookup(emb_lookup, X)\n",
    "\n",
    "# 对embedding使用RNN网络\n",
    "xavier_scale = 1 / math.sqrt(params.emb_size + params.lstm_size[-1]) / 3\n",
    "initializer = tf.random_uniform_initializer(-xavier_scale, xavier_scale)\n",
    "\n",
    "#####################\n",
    "lstm_outputs=gen_lstm_layer(emb,unit_I=params.emb_size,unit_O=params.lstm_size[0],\n",
    "                            t_size=params.t_size,batch_size=params.batch_size,init=initializer)\n",
    "#####################\n",
    "\n",
    "# with tf.variable_scope('LSTM', initializer=initializer):\n",
    "#     lstm_layers = list()\n",
    "#     for i in range(params.lstm_layers):\n",
    "#         layer = tf.nn.rnn_cell.LSTMCell(params.lstm_size[i])\n",
    "\n",
    "#         # DropoutWrapper没有training参数，只能使用tf.cond来实现\n",
    "#         keep_prob = tf.cond(is_training,\n",
    "#                             lambda: 1-params.dropout_rate,\n",
    "#                             lambda: tf.constant(1.0))\n",
    "#         layer = tf.nn.rnn_cell.DropoutWrapper(layer,\n",
    "#                                               output_keep_prob=keep_prob)\n",
    "\n",
    "#         lstm_layers.append(layer)\n",
    "\n",
    "#     lstm_layers = tf.nn.rnn_cell.MultiRNNCell(lstm_layers)\n",
    "\n",
    "#     lstm_outputs, _ = tf.nn.dynamic_rnn(lstm_layers,\n",
    "#                                         inputs=emb, dtype=tf.float32)\n",
    "#     lstm_outputs = lstm_outputs[:, -1, :]\n",
    "\n",
    "with tf.name_scope('FC'):\n",
    "    fc = tf.layers.dense(lstm_outputs, params.fc_size, activation=tf.nn.relu)\n",
    "    fc = tf.layers.dropout(fc, rate=params.dropout_rate, training=is_training)\n",
    "\n",
    "logits = tf.layers.dense(fc, unit_O,\n",
    "                         activation=None)    # 后接FC层，无激活\n",
    "\n",
    "with tf.name_scope('Eval'):\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=Y, logits=logits)\n",
    "    predict = tf.argmax(logits, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predict, Y), tf.float32))\n",
    "\n",
    "with tf.name_scope('train_op'):\n",
    "    lr = 1e-3\n",
    "    t_vars = tf.trainable_variables()    # 可训练变量\n",
    "    # 应用梯度截断\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, t_vars),\n",
    "                                      params.grad_thresh)\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    train_op = optimizer.apply_gradients(zip(grads, t_vars),\n",
    "                                         global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True    # 按需使用显存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, batch_loss: 1.1625792980194092, batch_acc: 0.5\n",
      "epoch: 2, batch_loss: 0.9158648252487183, batch_acc: 0.6875\n",
      "epoch: 3, batch_loss: 0.6839292049407959, batch_acc: 0.78125\n",
      "epoch: 5, batch_loss: 0.31747156381607056, batch_acc: 0.90625\n",
      "epoch: 6, batch_loss: 0.24026355147361755, batch_acc: 0.921875\n",
      "epoch: 6, test_acc: 0.8167067170143127\n",
      "epoch: 7, batch_loss: 0.1479153037071228, batch_acc: 0.921875\n",
      "epoch: 8, batch_loss: 0.12282991409301758, batch_acc: 0.953125\n",
      "epoch: 10, batch_loss: 0.07064007222652435, batch_acc: 0.984375\n",
      "epoch: 11, batch_loss: 0.05656968057155609, batch_acc: 0.984375\n",
      "epoch: 12, batch_loss: 0.03091406263411045, batch_acc: 0.984375\n",
      "epoch: 12, test_acc: 0.8397436141967773\n",
      "epoch: 14, batch_loss: 0.11046174168586731, batch_acc: 0.953125\n",
      "epoch: 15, batch_loss: 0.025200583040714264, batch_acc: 0.984375\n",
      "epoch: 16, batch_loss: 0.010227980092167854, batch_acc: 1.0\n",
      "epoch: 17, batch_loss: 0.04418171942234039, batch_acc: 0.984375\n",
      "epoch: 19, batch_loss: 0.004938417114317417, batch_acc: 1.0\n",
      "epoch: 19, test_acc: 0.8457531929016113\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    epochs = 20\n",
    "\n",
    "    batch_cnt = 0\n",
    "    for epoch in range(epochs):\n",
    "        for batch_data, batch_labels in train_data.next_batch():\n",
    "            batch_cnt += 1\n",
    "            loss_val, acc_val, _ = sess.run(\n",
    "                [loss, accuracy, train_op], feed_dict={\n",
    "                    X: batch_data,\n",
    "                    Y: batch_labels,\n",
    "                    is_training: True\n",
    "                })\n",
    "\n",
    "            # 每1000batch输出一次信息\n",
    "            if (batch_cnt+1) % 1000 == 0:\n",
    "                print('epoch: {}, batch_loss: {}, batch_acc: {}'.format(\n",
    "                    epoch, loss_val, acc_val))\n",
    "\n",
    "            # 每5000batch做一次验证\n",
    "            if (batch_cnt+1) % 5000 == 0:\n",
    "                all_test_acc_val = list()\n",
    "                for test_batch_data, test_batch_labels in val_data.next_batch():\n",
    "                    test_acc_val = sess.run(accuracy, feed_dict={\n",
    "                        X: test_batch_data,\n",
    "                        Y: test_batch_labels,\n",
    "                        is_training: False\n",
    "                    })\n",
    "                    all_test_acc_val.append(test_acc_val)\n",
    "                test_acc = np.mean(all_test_acc_val)\n",
    "                print('epoch: {}, test_acc: {}'.format(epoch, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
