{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/text8', 'r', encoding='utf-8') as fd:\n",
    "    words = fd.read().split()\n",
    "    \n",
    "# words=words[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from collections import Counter\n",
    "\n",
    "voc_size = 50000    # 词典大小\n",
    "\n",
    "word_cnt = list()\n",
    "word_cnt.extend(Counter(words).most_common(voc_size-1))    # -1为未在记录的词UNK预留一个位置\n",
    "\n",
    "# 映射表，记得把0预留给UNK\n",
    "word2int = {item[0]: idx+1 for idx, item in enumerate(word_cnt)}\n",
    "int2word = {idx+1: item[0] for idx, item in enumerate(word_cnt)}\n",
    "\n",
    "data = list(map(lambda x: word2int.get(x, 0), words))    # 将所有word转成int\n",
    "\n",
    "unk_cnt = len(words)-reduce(lambda x, y: x+y, map(lambda x: x[1], word_cnt))\n",
    "word_cnt.insert(0, ('UNK', unk_cnt))    # 未在记录的词\n",
    "\n",
    "# 为映射表添加UNK\n",
    "word2int['UNK'] = 0\n",
    "int2word[0] = 'UNK'\n",
    "\n",
    "del words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用滑动窗口生成分批数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class WordData:\n",
    "    def __init__(self, words, batch_size=32, cent_offset=2, cont_per_cent=4):\n",
    "        self.words = words\n",
    "        self.batch_size = batch_size\n",
    "        self.cent_offset = cent_offset    # 中心词的在窗口中的idx，即左右窗口的大小\n",
    "        self.cont_per_cent = cont_per_cent    # 每个中心词产生4个上下文，即4个样本\n",
    "\n",
    "        # batch_size是每个中心词产生样本数量的整数倍，这样就保证了每生成一个batch就会改变中心词\n",
    "        assert self.batch_size % self.cont_per_cent == 0\n",
    "        assert self.cont_per_cent <= self.cent_offset*2    # 每个中心词生成样本数应小于等于窗口内的上下文单词数\n",
    "\n",
    "        self.sample_cnt = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        self.data = list()\n",
    "        self.label = list()\n",
    "\n",
    "        for idx, center_word in enumerate(self.words):\n",
    "            for context_word in self.words[max(0, idx-self.cent_offset):min(idx+self.cent_offset, len(self.words))+1]:\n",
    "                if context_word != center_word:\n",
    "                    self.data.append(center_word)\n",
    "                    self.label.append(context_word)\n",
    "\n",
    "                    self.sample_cnt += 1    # 每生成一个样本进行计数\n",
    "\n",
    "                    if self.sample_cnt == self.batch_size:    # 样本数达到一个batch时抛出\n",
    "                        self.sample_cnt = 0\n",
    "                        yield np.array(self.data), np.array(self.label).reshape((-1,1))\n",
    "                        self.data = list()\n",
    "                        self.label = list()\n",
    "\n",
    "        yield np.array(self.data), np.array(self.label).reshape((-1,1))    # 抛出多余数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "train_data=WordData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_I = 1    # 单个数字表示的word\n",
    "emb_size = 128\n",
    "unit_O = 1\n",
    "\n",
    "neg_samples = 10    # 负采样参数\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "X = tf.placeholder(tf.int32, shape=[None])\n",
    "Y = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "with tf.name_scope('Emb'):\n",
    "    emb = tf.Variable(tf.random_uniform([voc_size, emb_size], -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(emb, X)\n",
    "\n",
    "with tf.name_scope('Eval'):\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [voc_size, emb_size],\n",
    "            stddev=1.0 / math.sqrt(emb_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([voc_size]))\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(\n",
    "            weights=nce_weights,\n",
    "            biases=nce_biases,\n",
    "            labels=Y,\n",
    "            inputs=embed,\n",
    "            num_sampled=neg_samples,\n",
    "            num_classes=voc_size))\n",
    "\n",
    "with tf.name_scope('train_op'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "with tf.name_scope('Valid'):\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(emb), 1, keepdims=True))\n",
    "    norm_emb = emb / norm\n",
    "    valid_emb = tf.nn.embedding_lookup(norm_emb, valid_dataset)\n",
    "    similarity = tf.matmul(valid_emb, norm_emb, transpose_b=True)\n",
    "\n",
    "init = tf.global_variables_initializer()    # 所有变量初始化\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True    # 按需使用显存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'average_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4f4ad8409fcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 feed_dict={X: batch_data, Y: batch_labels})\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0maverage_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# 每1000batch输出一次信息\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'average_loss' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    epochs = 20\n",
    "\n",
    "    batch_cnt = 0\n",
    "    for epoch in range(epochs):\n",
    "        for batch_data, batch_labels in train_data.next_batch():\n",
    "            batch_cnt += 1\n",
    "            _, loss_val = sess.run(\n",
    "                [optimizer, loss],\n",
    "                feed_dict={X: batch_data, Y: batch_labels})\n",
    "\n",
    "            # 每1000batch输出一次信息\n",
    "            if (batch_cnt+1) % 1000 == 0:\n",
    "                print('epoch: {}, batch_loss: {}'.format(\n",
    "                    epoch+1, loss_val))\n",
    "                \n",
    "            if (batch_cnt+1) % 5000 == 0:\n",
    "                sim=similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_word=int2word[valid_examples[i]]\n",
    "                    top_k=5\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                    log='Nearest to {}:'.format(valid_word)\n",
    "                    for k in range(top_k):\n",
    "                        sim_word=int2word[nearest[k]]\n",
    "                        log+=' {}'.format(sim_word)\n",
    "                    print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
