{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import struct\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    with open(path, 'rb') as fd:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII', fd.read(16))\n",
    "        res = np.fromfile(fd, dtype=np.uint8).reshape(-1, 784)\n",
    "    return res\n",
    "\n",
    "\n",
    "def load_label(path):\n",
    "    with open(path, 'rb') as fd:\n",
    "        magic, n = struct.unpack('>II', fd.read(8))\n",
    "        res = np.fromfile(fd, dtype=np.uint8)\n",
    "    return res\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class MnistData:\n",
    "    def __init__(self, data_path,label_path, batch_size=32, normalize=False, shuffle=False):\n",
    "        '''\n",
    "        paths: 文件路径\n",
    "        '''\n",
    "        self._data = list()\n",
    "        self._target = list()\n",
    "        self._n_samples = 0\n",
    "        self.n_features = 0\n",
    "\n",
    "        self._idx = 0    # mini-batch的游标\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        self._load(data_path,label_path)\n",
    "\n",
    "        if shuffle:\n",
    "            self._shuffle_data()\n",
    "        if normalize:\n",
    "            self._normalize_data()\n",
    "\n",
    "        print(self._data.shape, self._target.shape)\n",
    "        \n",
    "    def _load(self, data_path,label_path):\n",
    "        '''\n",
    "        载入数据\n",
    "        '''\n",
    "        self._data=load_image(data_path)\n",
    "        self._target=load_label(label_path)\n",
    "\n",
    "        self._n_samples, self.n_features = self._data.shape[0], self._data.shape[1]\n",
    "        \n",
    "    def _shuffle_data(self):\n",
    "        '''\n",
    "        打乱数据\n",
    "        '''\n",
    "        idxs = np.random.permutation(self._n_samples)\n",
    "        self._data = self._data[idxs]\n",
    "        self._target = self._target[idxs]\n",
    "\n",
    "    def _normalize_data(self):\n",
    "        scaler = StandardScaler()\n",
    "        self._data = scaler.fit_transform(self._data)\n",
    "\n",
    "    def next_batch(self):\n",
    "        '''\n",
    "        生成mini-batch\n",
    "        '''\n",
    "        while self._idx < self._n_samples:\n",
    "            yield self._data[self._idx: (self._idx+self._batch_size)], self._target[self._idx: (self._idx+self._batch_size)]\n",
    "            self._idx += self._batch_size\n",
    "\n",
    "        self._idx = 0\n",
    "        self._shuffle_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "MNIST_DIR = '../dataset/MNIST/'\n",
    "train_data_path = os.path.join(MNIST_DIR, 'train-images.idx3-ubyte')\n",
    "train_label_path = os.path.join(MNIST_DIR, 'train-labels.idx1-ubyte')\n",
    "test_data_path = os.path.join(MNIST_DIR, 't10k-images.idx3-ubyte')\n",
    "test_label_path = os.path.join(MNIST_DIR, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "batch_size = 32\n",
    "train_data = MnistData(train_data_path,train_label_path, batch_size=batch_size,\n",
    "                       normalize=False, shuffle=True)\n",
    "test_data = MnistData(test_data_path,test_label_path, batch_size=batch_size,\n",
    "                      normalize=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络结构设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_I = 28\n",
    "n_steps = 28    # 状态数\n",
    "unit_h = 256\n",
    "unit_O = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-1433cbcd4f8a>:14: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-1433cbcd4f8a>:22: static_bidirectional_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell, unroll=True))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py:1565: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-1433cbcd4f8a>:25: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_steps, unit_I])\n",
    "Y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "# RNN网络图\n",
    "with tf.name_scope('RNN'):\n",
    "    X_seq = tf.transpose(X, [1, 0, 2])    # 把状态移到第一维\n",
    "    X_seq = tf.reshape(X_seq, [-1, unit_I])\n",
    "    X_seq = tf.split(X_seq, n_steps)\n",
    "\n",
    "    lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(unit_h)    # 前向单元\n",
    "    lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(unit_h)    # 反向单元\n",
    "\n",
    "    # 双向循环单元\n",
    "    outputs, _, _ = tf.nn.static_bidirectional_rnn(\n",
    "        cell_fw=lstm_fw_cell,\n",
    "        cell_bw=lstm_bw_cell,\n",
    "        inputs=X_seq,\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    \n",
    "    logits=tf.layers.dense(outputs[-1], unit_O, activation=None)\n",
    "\n",
    "# 评估图\n",
    "with tf.name_scope('Eval'):\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=Y, logits=logits)\n",
    "    predict = tf.argmax(logits, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predict, Y), tf.float32))\n",
    "\n",
    "# 优化图\n",
    "with tf.name_scope('train_op'):\n",
    "    lr = 1e-3\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True    # 按需使用显存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch_loss: 0.3887373208999634, batch_acc: 0.8125\n",
      "epoch: 1, batch_loss: 0.2442668378353119, batch_acc: 0.84375\n",
      "epoch: 1, batch_loss: 0.1861989051103592, batch_acc: 0.9375\n",
      "epoch: 2, batch_loss: 0.26198431849479675, batch_acc: 0.90625\n",
      "epoch: 2, batch_loss: 0.2022436261177063, batch_acc: 0.9375\n",
      "epoch: 2, test_acc: 0.9263178706169128\n",
      "epoch: 3, batch_loss: 0.32303673028945923, batch_acc: 0.9375\n",
      "epoch: 3, batch_loss: 0.3292866349220276, batch_acc: 0.90625\n",
      "epoch: 4, batch_loss: 0.0797535702586174, batch_acc: 1.0\n",
      "epoch: 4, batch_loss: 0.2004379779100418, batch_acc: 0.90625\n",
      "epoch: 5, batch_loss: 0.10955925285816193, batch_acc: 0.96875\n",
      "epoch: 5, test_acc: 0.9399960041046143\n",
      "epoch: 5, batch_loss: 0.06019783765077591, batch_acc: 0.96875\n",
      "epoch: 6, batch_loss: 0.09795217961072922, batch_acc: 0.96875\n",
      "epoch: 6, batch_loss: 0.20790743827819824, batch_acc: 0.9375\n",
      "epoch: 7, batch_loss: 0.14399084448814392, batch_acc: 0.9375\n",
      "epoch: 7, batch_loss: 0.10233209282159805, batch_acc: 0.96875\n",
      "epoch: 7, test_acc: 0.9435902833938599\n",
      "epoch: 8, batch_loss: 0.056579556316137314, batch_acc: 1.0\n",
      "epoch: 9, batch_loss: 0.020774543285369873, batch_acc: 1.0\n",
      "epoch: 9, batch_loss: 0.07201466709375381, batch_acc: 0.96875\n",
      "epoch: 10, batch_loss: 0.30993029475212097, batch_acc: 0.90625\n",
      "epoch: 10, batch_loss: 0.10270514339208603, batch_acc: 0.9375\n",
      "epoch: 10, test_acc: 0.9530750513076782\n",
      "epoch: 11, batch_loss: 0.07003212720155716, batch_acc: 0.96875\n",
      "epoch: 11, batch_loss: 0.14169654250144958, batch_acc: 0.9375\n",
      "epoch: 12, batch_loss: 0.05987148731946945, batch_acc: 0.9375\n",
      "epoch: 12, batch_loss: 0.281363844871521, batch_acc: 0.9375\n",
      "epoch: 13, batch_loss: 0.0726909264922142, batch_acc: 0.9375\n",
      "epoch: 13, test_acc: 0.9545726776123047\n",
      "epoch: 13, batch_loss: 0.014387380331754684, batch_acc: 1.0\n",
      "epoch: 14, batch_loss: 0.17447316646575928, batch_acc: 0.9375\n",
      "epoch: 14, batch_loss: 0.016006087884306908, batch_acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    epochs = 20\n",
    "\n",
    "    batch_cnt = 0\n",
    "    for epoch in range(epochs):\n",
    "        for batch_data, batch_labels in train_data.next_batch():\n",
    "            batch_data = batch_data.reshape((-1, n_steps, unit_I))\n",
    "            batch_cnt += 1\n",
    "            loss_val, acc_val, _ = sess.run(\n",
    "                [loss, accuracy, train_op],\n",
    "                feed_dict={\n",
    "                    X: batch_data,\n",
    "                    Y: batch_labels})\n",
    "\n",
    "            # 每1000batch输出一次信息\n",
    "            if (batch_cnt+1) % 1000 == 0:\n",
    "                print('epoch: {}, batch_loss: {}, batch_acc: {}'.format(\n",
    "                    epoch, loss_val, acc_val))\n",
    "\n",
    "            # 每5000batch做一次验证\n",
    "            if (batch_cnt+1) % 5000 == 0:\n",
    "                all_test_acc_val = list()\n",
    "                for test_batch_data, test_batch_labels in test_data.next_batch():\n",
    "                    test_batch_data = test_batch_data.reshape((-1, n_steps, unit_I))\n",
    "                    test_acc_val = sess.run(\n",
    "                        [accuracy],\n",
    "                        feed_dict={\n",
    "                            X: test_batch_data,\n",
    "                            Y: test_batch_labels\n",
    "                        })\n",
    "                    all_test_acc_val.append(test_acc_val)\n",
    "                test_acc = np.mean(all_test_acc_val)\n",
    "                print('epoch: {}, test_acc: {}'.format(epoch, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
