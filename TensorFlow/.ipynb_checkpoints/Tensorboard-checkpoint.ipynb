{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.dirname(os.path.abspath('.')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard是TensorFlow提供的一可视化工具，这里修改```mini_CNN```的代码以做演示。\n",
    "\n",
    "首先需要构建好计算图："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072) (50000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072) (10000,)\n",
      "WARNING:tensorflow:From <ipython-input-2-f7986dc9c1c4>:26: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-2-f7986dc9c1c4>:28: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-f7986dc9c1c4>:40: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-2-f7986dc9c1c4>:40: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from dataset.dataset import load_cifar10\n",
    "\n",
    "\n",
    "train_data, test_data = load_cifar10(batch_size=64)\n",
    "\n",
    "unit_I = train_data.n_features\n",
    "\n",
    "filters = 32\n",
    "conv_size = (3, 3)\n",
    "\n",
    "pool_size = (2, 2)\n",
    "strides = (2, 2)\n",
    "\n",
    "unit_O = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, unit_I])\n",
    "Y = tf.placeholder(tf.int64, [None])\n",
    "X_img = tf.transpose(tf.reshape(\n",
    "    X, [-1, 3, 32, 32]), perm=[0, 2, 3, 1])\n",
    "\n",
    "with tf.name_scope('CNN'):\n",
    "    conv1 = tf.layers.conv2d(X_img, filters=filters,\n",
    "                             kernel_size=conv_size, padding='same',\n",
    "                             activation=tf.nn.relu, name='conv1')\n",
    "    pooling1 = tf.layers.max_pooling2d(conv1, pool_size=pool_size,\n",
    "                                       strides=strides, name='pooling1')\n",
    "    conv2 = tf.layers.conv2d(pooling1, filters=filters,\n",
    "                             kernel_size=conv_size, padding='same',\n",
    "                             activation=tf.nn.relu, name='conv2')\n",
    "    pooling2 = tf.layers.max_pooling2d(conv2, pool_size=pool_size,\n",
    "                                       strides=strides, name='pooling2')\n",
    "    conv3 = tf.layers.conv2d(pooling2, filters=filters,\n",
    "                             kernel_size=conv_size, padding='same',\n",
    "                             activation=tf.nn.relu, name='conv3')\n",
    "    pooling3 = tf.layers.max_pooling2d(conv3, pool_size=pool_size,\n",
    "                                       strides=strides, name='pooling3')\n",
    "    logits = tf.layers.dense(tf.layers.flatten(\n",
    "        pooling3), unit_O, activation=None)\n",
    "\n",
    "with tf.name_scope('Eval'):\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=Y, logits=logits)\n",
    "    predict = tf.argmax(logits, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predict, Y), tf.float32))\n",
    "\n",
    "with tf.name_scope('train_op'):\n",
    "    lr = 1e-3\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在构建好图之后，使用tensorboard相关API加入我们关心的变量。当然以下代码也可写在计算图中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_summary = tf.summary.scalar('loss', loss)    # loss\n",
    "acc_summary = tf.summary.scalar('acc', accuracy)    # acc\n",
    "img_summary = tf.summary.image('input_img', X_img)    # 图片型数据\n",
    "\n",
    "# 不同阶段关注的数据可能不同\n",
    "train_summary = tf.summary.merge_all()    # 训练阶段关注所有变量\n",
    "test_summary = tf.summary.merge([loss_summary,\n",
    "                                 acc_summary])    # 测试阶段只关注loss与acc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定输出的log文件夹："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_ROOT = '.'    # notebook的同级目录\n",
    "save_dir = os.path.join(LOG_ROOT, 'Tensorboard')    # notebook的同名文件夹\n",
    "if not os.path.exists(save_dir):    # 不存在则创建\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# train与test数据分开存放\n",
    "train_log_dir = os.path.join(save_dir, 'train')\n",
    "if not os.path.exists(train_log_dir):\n",
    "    os.makedirs(train_log_dir)\n",
    "\n",
    "test_log_dir = os.path.join(save_dir, 'test')\n",
    "if not os.path.exists(test_log_dir):\n",
    "    os.makedirs(test_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练代码中加入writer："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, batch_loss: 1.1910089254379272, batch_acc: 0.515625\n",
      "epoch: 2, batch_loss: 0.9810861945152283, batch_acc: 0.671875\n",
      "epoch: 3, batch_loss: 1.1511430740356445, batch_acc: 0.609375\n",
      "epoch: 5, batch_loss: 0.8800821900367737, batch_acc: 0.625\n",
      "epoch: 6, batch_loss: 0.6403199434280396, batch_acc: 0.765625\n",
      "epoch: 6, test_acc: 0.7093999981880188\n",
      "epoch: 7, batch_loss: 0.7179678678512573, batch_acc: 0.703125\n",
      "epoch: 8, batch_loss: 0.5536156296730042, batch_acc: 0.828125\n",
      "epoch: 10, batch_loss: 0.7096176147460938, batch_acc: 0.828125\n",
      "epoch: 11, batch_loss: 0.5687694549560547, batch_acc: 0.78125\n",
      "epoch: 12, batch_loss: 0.6487060785293579, batch_acc: 0.75\n",
      "epoch: 12, test_acc: 0.7311000227928162\n",
      "epoch: 14, batch_loss: 0.5611023902893066, batch_acc: 0.828125\n",
      "epoch: 15, batch_loss: 0.7122163772583008, batch_acc: 0.703125\n",
      "epoch: 16, batch_loss: 0.5313572883605957, batch_acc: 0.875\n",
      "epoch: 17, batch_loss: 0.49914979934692383, batch_acc: 0.8125\n",
      "epoch: 19, batch_loss: 0.40212172269821167, batch_acc: 0.828125\n",
      "epoch: 19, test_acc: 0.7190999984741211\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    epochs = 20\n",
    "\n",
    "    # 创建writer\n",
    "    train_writer = tf.summary.FileWriter(\n",
    "        train_log_dir, sess.graph)    # 将计算图写入训练文件夹\n",
    "    test_writer = tf.summary.FileWriter(test_log_dir)\n",
    "\n",
    "    # 这里关于测试集的summary比较麻烦，因为数据类是写成了生成器的模式，但是在写summary的过程无法使用循环\n",
    "    # 有两种方法，一种是提前取出一个测试batch去估计测试summary，而是将测试数据全部取出来\n",
    "\n",
    "    batch_cnt = 0\n",
    "    for epoch in range(epochs):\n",
    "        for batch_data, batch_labels in train_data.next_batch():\n",
    "            loss_val, acc_val, _ = sess.run(\n",
    "                [loss, accuracy, train_op],\n",
    "                feed_dict={\n",
    "                    X: batch_data,\n",
    "                    Y: batch_labels})\n",
    "            batch_cnt += 1\n",
    "\n",
    "            # 每100个batch写入一次summary\n",
    "            if (batch_cnt+1) % 100 == 0:\n",
    "                sum_res_train = sess.run(train_summary,\n",
    "                                         feed_dict={X: batch_data,\n",
    "                                                    Y: batch_labels})\n",
    "                train_writer.add_summary(sum_res_train, batch_cnt+1)\n",
    "\n",
    "                # 关键在于测试集\n",
    "                sum_res_test = sess.run(test_summary,\n",
    "                                        feed_dict={X: test_data.data,\n",
    "                                                   Y: test_data.target})\n",
    "                test_writer.add_summary(sum_res_test, batch_cnt+1)\n",
    "                \n",
    "    print('training done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进入```./Tensorboard/```目录，输入\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=train:'train',test:'test'\n",
    "```\n",
    "\n",
    "TensorBoard的默认web端口为```:6006```。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
