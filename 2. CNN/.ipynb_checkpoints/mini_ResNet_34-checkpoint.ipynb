{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    '''\n",
    "    CIFAR-10数据读取函数\n",
    "    '''\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fd:\n",
    "        data = pickle.load(fd, encoding='bytes')\n",
    "    return data[b'data'], np.array(data[b'labels'])\n",
    "\n",
    "\n",
    "class CifarData:\n",
    "    def __init__(self, paths, batch_size=32, normalize=False, shuffle=False):\n",
    "        '''\n",
    "        paths: 文件路径\n",
    "        '''\n",
    "        self._data = list()\n",
    "        self._target = list()\n",
    "        self._n_samples = 0\n",
    "        self.n_features = 0\n",
    "\n",
    "        self._idx = 0    # mini-batch的游标\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        self._load(paths)\n",
    "\n",
    "        if shuffle:\n",
    "            self._shuffle_data()\n",
    "        if normalize:\n",
    "            self._normalize_data()\n",
    "\n",
    "        print(self._data.shape, self._target.shape)\n",
    "\n",
    "    def _load(self, paths):\n",
    "        '''\n",
    "        载入数据\n",
    "        '''\n",
    "        for path in paths:\n",
    "            data, labels = unpickle(path)\n",
    "            self._data.append(data)\n",
    "            self._target.append(labels)\n",
    "\n",
    "        # 将所有批次的数据拼接起来\n",
    "        self._data, self._target = np.vstack(\n",
    "            self._data), np.hstack(self._target)\n",
    "\n",
    "        self._n_samples, self.n_features = self._data.shape[0], self._data.shape[1]\n",
    "\n",
    "    def _shuffle_data(self):\n",
    "        '''\n",
    "        打乱数据\n",
    "        '''\n",
    "        idxs = np.random.permutation(self._n_samples)\n",
    "        self._data = self._data[idxs]\n",
    "        self._target = self._target[idxs]\n",
    "\n",
    "    def _normalize_data(self):\n",
    "        scaler = StandardScaler()\n",
    "        self._data = scaler.fit_transform(self._data)\n",
    "\n",
    "    def next_batch(self):\n",
    "        '''\n",
    "        生成mini-batch\n",
    "        '''\n",
    "        while self._idx < self._n_samples:\n",
    "            yield self._data[self._idx: (self._idx+self._batch_size)], self._target[self._idx: (self._idx+self._batch_size)]\n",
    "            self._idx += self._batch_size\n",
    "\n",
    "        self._idx = 0\n",
    "        self._shuffle_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072) (50000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072) (10000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "CIFAR_DIR = \"../dataset/cifar-10-batches-py/\"\n",
    "train_filenames = [os.path.join(\n",
    "    CIFAR_DIR, 'data_batch_{}'.format(i)) for i in range(1, 6)]\n",
    "test_filenames = [os.path.join(CIFAR_DIR, 'test_batch')]\n",
    "\n",
    "batch_size = 32\n",
    "train_data = CifarData(train_filenames, batch_size=batch_size,\n",
    "                       normalize=True, shuffle=True)\n",
    "test_data = CifarData(test_filenames, batch_size=batch_size,\n",
    "                      normalize=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络结构设计\n",
    "原论文中resnet34中不包含bottleneck层，并且每经过一个残差块数据流的尺寸减半、深度加倍。另外，由于这里使用的是CIFAR10数据集，尺寸为$32{\\times}32$，无法承受多次缩小，这里就省略了最开始的卷积层$(7{\\times}7{\\times}64)$与最大池化层$(3{\\times}3{\\times}64)$，使用一个$(3{\\times}3{\\times}32)$、步长1的卷积核代替。即这里实现的ResNet是从conv3才开始减少尺寸的。\n",
    "\n",
    "首先定义一个构建残差块的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_I = train_data.n_features    # 输入单元数，等于特征数\n",
    "\n",
    "conv_size = (3, 3)    # 卷积核尺寸\n",
    "init_filters=32    # 初始卷积核数量\n",
    "\n",
    "# 两种步长\n",
    "strides_1 = (1, 1)\n",
    "strides_2 = (2, 2)\n",
    "\n",
    "# 池化核尺寸\n",
    "pool_size = (2, 2)\n",
    "\n",
    "n_res_blocks=[3,4,6,3]    # 各部分残差块的数量\n",
    "the_last_layer=None    # 用于缓存\n",
    "\n",
    "fc_size = 128    # 全连接层单元数\n",
    "\n",
    "unit_O = 10    # 输出单元数，类别数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_block(inputs, out_channels, conv_id=2,block_id=1):\n",
    "    '''\n",
    "    inputs: 输入\n",
    "    out_channels: 输出数据的通道数\n",
    "    '''\n",
    "    in_channels = inputs.get_shape().as_list()[-1]    # 最后一维为深度\n",
    "    if out_channels == in_channels*2:\n",
    "        deepen = True\n",
    "        strides = strides_2    # 加深的同时尺寸减半，通过步长来实现\n",
    "    else:\n",
    "        deepen = False\n",
    "        strides = strides_1\n",
    "\n",
    "    with tf.variable_scope('conv{}'.format(conv_id)):\n",
    "        # 如果需要降采样(减半尺寸)，论文中指明只在残差块的第一层中使用步长为2的卷积核实现\n",
    "        conv1 = tf.layers.conv2d(inputs, filters=out_channels,\n",
    "                                 kernel_size=conv_size, strides=strides, padding='same',\n",
    "                                 activation=tf.nn.relu, name='conv{}-{}-1'.format(conv_id,block_id))\n",
    "        # 后续步长为1\n",
    "        conv2 = tf.layers.conv2d(conv1, filters=out_channels,\n",
    "                                 kernel_size=conv_size, strides=strides_1,  padding='same',\n",
    "                                 activation=tf.nn.relu, name='conv{}-{}-2'.format(conv_id,block_id))\n",
    "\n",
    "    # 如果做了加深操作，那么相加前需要对inputs做同样的变换：减半加深\n",
    "    if deepen:\n",
    "        # 尺寸减半\n",
    "        input_trans = tf.layers.average_pooling2d(inputs, pool_size=pool_size,\n",
    "                                                  strides=strides_2)\n",
    "        # 深度加倍，只对最后一维(深度)填充\n",
    "        input_trans = tf.pad(input_trans,\n",
    "                             [[0, 0], [0, 0], [0, 0], [in_channels//2, in_channels//2]])\n",
    "    else:\n",
    "        input_trans = inputs\n",
    "\n",
    "    return input_trans+conv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后是网络结构的创建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-70490fd1de50>:10: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-72a843c62c4f>:28: average_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.average_pooling2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-70490fd1de50>:26: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-70490fd1de50>:27: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, unit_I])  # 数据的样本数不指定，只指定特征数\n",
    "Y = tf.placeholder(tf.int64, [None])    # 目标值为列向量，int64为了兼容\n",
    "X_img = tf.transpose(tf.reshape(X, [-1, 3, 32, 32]),\n",
    "                     perm=[0, 2, 3, 1])    # 转为图片格式送入模型，(n_samples,width,height,depth)\n",
    "\n",
    "# 第一部分，使用3*3、步长1的卷积代替最开始的卷积+池化\n",
    "with tf.variable_scope('conv1'):\n",
    "    conv1 = tf.layers.conv2d(X_img, init_filters,\n",
    "                             kernel_size=conv_size, strides=strides_1, padding='same',\n",
    "                             activation=tf.nn.relu, name='conv1-1')\n",
    "    the_last_layer = conv1\n",
    "\n",
    "# 残差的4个部分：conv2,conv3,conv4,conv5\n",
    "for conv_id in range(4):\n",
    "    # 子编号，如conv2-1,conv3-2等\n",
    "    for block_id in range(n_res_blocks[conv_id]):\n",
    "        cur_conv = res_block(the_last_layer,\n",
    "                             # 深度(卷积核数)以2的倍数增加\n",
    "                             init_filters * (2**(conv_id)),\n",
    "                             conv_id+2, block_id+1)\n",
    "        the_last_layer = cur_conv\n",
    "\n",
    "# 这里使用求平均的方法实现了一个通用的全局平均池化\n",
    "with tf.variable_scope('FC'):\n",
    "    global_pool = tf.reduce_mean(the_last_layer, [1, 2])\n",
    "    fc = tf.layers.dense(tf.layers.flatten(global_pool),\n",
    "                         fc_size, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(global_pool, unit_O, activation=None)\n",
    "    \n",
    "# 评估图\n",
    "with tf.name_scope('Eval'):\n",
    "    # 计算一维向量与onehot向量之间的损失\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=Y, logits=logits)\n",
    "    predict = tf.argmax(logits, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predict, Y), tf.float32))\n",
    "\n",
    "# 优化图\n",
    "with tf.name_scope('train_op'):\n",
    "    lr = 1e-3\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True    # 按需使用显存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch_loss: 1.8687684535980225, batch_acc: 0.46875\n",
      "epoch: 1, batch_loss: 1.144719123840332, batch_acc: 0.5625\n",
      "epoch: 1, batch_loss: 0.856376051902771, batch_acc: 0.75\n",
      "epoch: 2, batch_loss: 0.8839129209518433, batch_acc: 0.6875\n",
      "epoch: 3, batch_loss: 0.7278085947036743, batch_acc: 0.78125\n",
      "epoch: 3, test_acc: 0.7395167946815491\n",
      "epoch: 3, batch_loss: 1.0069348812103271, batch_acc: 0.71875\n",
      "epoch: 4, batch_loss: 0.697412371635437, batch_acc: 0.75\n",
      "epoch: 5, batch_loss: 0.2498096525669098, batch_acc: 0.90625\n",
      "epoch: 5, batch_loss: 0.19883327186107635, batch_acc: 0.9375\n",
      "epoch: 6, batch_loss: 0.36449384689331055, batch_acc: 0.90625\n",
      "epoch: 6, test_acc: 0.7602835297584534\n",
      "epoch: 7, batch_loss: 0.4332618713378906, batch_acc: 0.78125\n",
      "epoch: 7, batch_loss: 0.12322409451007843, batch_acc: 0.90625\n",
      "epoch: 8, batch_loss: 0.17948602139949799, batch_acc: 0.96875\n",
      "epoch: 8, batch_loss: 0.32270288467407227, batch_acc: 0.90625\n",
      "epoch: 9, batch_loss: 0.28153201937675476, batch_acc: 0.875\n",
      "epoch: 9, test_acc: 0.7725638747215271\n",
      "epoch: 10, batch_loss: 0.24668145179748535, batch_acc: 0.90625\n",
      "epoch: 10, batch_loss: 0.06109265238046646, batch_acc: 1.0\n",
      "epoch: 11, batch_loss: 0.16481855511665344, batch_acc: 0.90625\n",
      "epoch: 12, batch_loss: 0.16027966141700745, batch_acc: 0.9375\n",
      "epoch: 12, batch_loss: 0.0349062979221344, batch_acc: 1.0\n",
      "epoch: 12, test_acc: 0.7663738131523132\n",
      "epoch: 13, batch_loss: 0.4490305483341217, batch_acc: 0.875\n",
      "epoch: 14, batch_loss: 0.038443244993686676, batch_acc: 1.0\n",
      "epoch: 14, batch_loss: 0.050197478383779526, batch_acc: 1.0\n",
      "epoch: 15, batch_loss: 0.09507792443037033, batch_acc: 0.9375\n",
      "epoch: 15, batch_loss: 0.27214112877845764, batch_acc: 0.90625\n",
      "epoch: 15, test_acc: 0.7654752135276794\n",
      "epoch: 16, batch_loss: 0.12921367585659027, batch_acc: 0.96875\n",
      "epoch: 17, batch_loss: 0.05582622438669205, batch_acc: 0.96875\n",
      "epoch: 17, batch_loss: 0.11568671464920044, batch_acc: 0.96875\n",
      "epoch: 18, batch_loss: 0.06694518029689789, batch_acc: 0.96875\n",
      "epoch: 19, batch_loss: 0.1421966254711151, batch_acc: 0.9375\n",
      "epoch: 19, test_acc: 0.7646765112876892\n",
      "epoch: 19, batch_loss: 0.07794512808322906, batch_acc: 0.96875\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    epochs = 20\n",
    "\n",
    "    batch_cnt = 0\n",
    "    for epoch in range(epochs):\n",
    "        for batch_data, batch_labels in train_data.next_batch():\n",
    "            batch_cnt += 1\n",
    "            loss_val, acc_val, _ = sess.run(\n",
    "                [loss, accuracy, train_op],\n",
    "                feed_dict={\n",
    "                    X: batch_data,\n",
    "                    Y: batch_labels})\n",
    "\n",
    "            # 每1000batch输出一次信息\n",
    "            if (batch_cnt+1) % 1000 == 0:\n",
    "                print('epoch: {}, batch_loss: {}, batch_acc: {}'.format(\n",
    "                    epoch, loss_val, acc_val))\n",
    "\n",
    "            # 每5000batch做一次验证\n",
    "            if (batch_cnt+1) % 5000 == 0:\n",
    "                all_test_acc_val = list()\n",
    "                for test_batch_data, test_batch_labels in test_data.next_batch():\n",
    "                    test_acc_val = sess.run(\n",
    "                        [accuracy],\n",
    "                        feed_dict={\n",
    "                            X: test_batch_data,\n",
    "                            Y: test_batch_labels\n",
    "                        })\n",
    "                    all_test_acc_val.append(test_acc_val)\n",
    "                test_acc = np.mean(all_test_acc_val)\n",
    "                print('epoch: {}, test_acc: {}'.format(epoch, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样不完整的ResNet要略优于VGGNet。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
