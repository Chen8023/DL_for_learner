{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledgement: https://gist.github.com/karpathy/d4dee566867f8291f086."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_set = set()\n",
    "# text = open('cano-trim.txt', 'r').read()\n",
    "text = 'a b c d e f g h i j k l m n o p'\n",
    "items = text.split()\n",
    "uni_set.update(items)\n",
    "# print(uni_set)\n",
    "\n",
    "# 元素-数字的双向映射表\n",
    "item2ind = {item: idx for idx, item in enumerate(uni_set)}\n",
    "ind2item = {idx: item for idx, item in enumerate(uni_set)}\n",
    "\n",
    "\n",
    "def label_encoder(item):\n",
    "    return item2ind[item]\n",
    "\n",
    "\n",
    "def label_decoder(num):\n",
    "    return ind2item[num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h_{t}=tanh(W_{x}x_{t}+W_{h_{t-1}}h_{t-1}+b_{h_{t-1}}) \\\\\n",
    "y_{t}=W_{h_{t}}h_{t}+b_{h_{t}} \\\\\n",
    "$$\n",
    "\n",
    "保持与之前DNN实现上的一致，易得三个权重矩阵的维度分别为：\n",
    "\n",
    "$$\n",
    "W_{x}=(x,h) \\\\\n",
    "W_{h_{t-1}}=(h,h) \\\\\n",
    "W_{h_{t}}=(h,y) \\\\\n",
    "$$\n",
    "\n",
    "参数设置与初始化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "unit_I = len(uni_set)    # 输入层单元数，等于数据特征数。此处用的是one-hot向量\n",
    "unit_h = 100    # 隐藏层单元数\n",
    "unit_O = unit_I    # 输出层单元数\n",
    "\n",
    "t_size = 5    # 状态数，即时间窗口大小\n",
    "\n",
    "\n",
    "params = OrderedDict({\n",
    "    'W_x': np.random.randn(unit_I, unit_h)*0.01,\n",
    "    'W_h_pre': np.random.randn(unit_h, unit_h)*0.01,\n",
    "    'b_h_pre': np.zeros((1, unit_h)),\n",
    "    'W_h_cur': np.random.randn(unit_h, unit_O)*0.01,\n",
    "    'b_h_cur': np.zeros((1, unit_O)),\n",
    "})\n",
    "\n",
    "h_pre = np.zeros((1, unit_h))    # h_{t-1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先是前向传播，因为RNN的一个样本有多个有序状态，所以一个样本的loss是该样本所有状态loss的累加："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_row(row_data):\n",
    "    return np.exp(row_data)/np.sum(np.exp(row_data))\n",
    "\n",
    "\n",
    "def forward_prop(x, y, h_pre):\n",
    "    '''\n",
    "    x: 数字表示的字符串\n",
    "    y: 数字表示的字符串，由x右移一位得到\n",
    "    h_pre: 隐层的上一个状态\n",
    "    '''\n",
    "    x_s = np.zeros((t_size, unit_I))    # 使用矩阵保存x当前轮的所有状态，一行为一个状态下的one-hot向量\n",
    "    h_s = np.zeros((t_size+1, unit_h))    # 使用最后一行保存上一轮的隐层状态，使用-1调用\n",
    "    h_s[-1] = h_pre\n",
    "    y_pred_s = np.zeros((t_size, unit_O))    # score\n",
    "    logit_s = np.zeros((t_size, unit_O))    # 下一个字符的概率向量\n",
    "    loss = 0\n",
    "\n",
    "    for t in range(t_size):    # 串行计算所有状态下的变量\n",
    "        x_s[t][x[t]] = 1    # one-hot向量的对应位置置1\n",
    "        h_s[t] = np.tanh(np.dot(x_s[t], params['W_x'])+np.dot(h_s[t-1],\n",
    "                                                              params['W_h_pre'])+params['b_h_pre'])    # 隐层状态\n",
    "        logit_s[t] = softmax_row(\n",
    "            np.dot(h_s[t], params['W_h_cur'])+params['b_h_cur'])    # softmax输出\n",
    "        loss += -np.log(logit_s[t][y[t]])    # 概率向量中，真实标签idx对应位置的值即为损失\n",
    "\n",
    "    cache = {\n",
    "        'logit_s': logit_s,\n",
    "        'h_s': h_s,\n",
    "        'x_s': x_s\n",
    "    }\n",
    "    return loss, cache\n",
    "\n",
    "\n",
    "# test\n",
    "# x = data[:t_size]\n",
    "# y = data[1:t_size+1]\n",
    "# h_pre = np.zeros((1, unit_h))\n",
    "# loss, cache=forward_prop(x, y, h_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(cache, x, y):\n",
    "    logit_s, h_s, x_s = cache['logit_s'], cache['h_s'], cache['x_s']\n",
    "\n",
    "    # loss关于参数的梯度\n",
    "    grads = OrderedDict({\n",
    "        'W_x': np.zeros_like(params['W_x']),\n",
    "        'W_h_pre': np.zeros_like(params['W_h_pre']),\n",
    "        'b_h_pre': np.zeros_like(params['b_h_pre']),\n",
    "        'W_h_cur': np.zeros_like(params['W_h_cur']),\n",
    "        'b_h_cur': np.zeros_like(params['b_h_cur']),\n",
    "    })\n",
    "\n",
    "    dh_post = np.zeros_like(h_s[0])    # loss对h_{t+1}的梯度\n",
    "\n",
    "    for t in range(t_size-1, -1, -1):    # 倒推\n",
    "        dlogit = np.copy(logit_s[t])\n",
    "        dlogit[y[t]] -= 1\n",
    "\n",
    "        grads['W_h_cur'] += np.dot(np.mat(h_s[t]).T, np.mat(dlogit))    # 向量转矩阵\n",
    "        grads['b_h_cur'] += dlogit\n",
    "\n",
    "        # 后一状态的dh_post是对当前层dh_cur有贡献的\n",
    "        dh_cur = np.dot(dlogit, params['W_h_cur'].T) + dh_post\n",
    "        dh_pre = (1-h_s[t]*h_s[t])*dh_cur\n",
    "\n",
    "        grads['b_h_pre'] += dh_pre\n",
    "        grads['W_h_pre'] += np.dot(h_s[t-1].T, dh_pre)\n",
    "        grads['W_x'] += np.dot(np.mat(x_s[t]).T, np.mat(dh_pre))\n",
    "\n",
    "        dh_post = np.dot(dh_pre, params['W_h_pre'].T)    # 更新后一状态的dh_post\n",
    "\n",
    "    # 梯度截断\n",
    "    for grad in grads.values():\n",
    "        np.clip(grad, -5, 5, out=grad)\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "# test\n",
    "# grads = backward_prop(cache, x, y)\n",
    "# for param in params.keys():\n",
    "#     assert grads[param].shape==params[param].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 iter: loss:17.587233118373\n",
      "20 iter: loss:17.131568532467412\n",
      "30 iter: loss:3.8492630331488606\n",
      "40 iter: loss:0.8659538325958968\n",
      "50 iter: loss:0.4009641501380095\n",
      "60 iter: loss:0.24821963374994838\n",
      "70 iter: loss:0.1752366606491887\n",
      "80 iter: loss:0.13385097567642673\n",
      "90 iter: loss:0.1075293399826527\n",
      "100 iter: loss:0.08945077639420426\n"
     ]
    }
   ],
   "source": [
    "max_iter = 100\n",
    "lr = 0.1\n",
    "\n",
    "# AdaGrad的累计平方梯度\n",
    "R_AdaGrad = OrderedDict({\n",
    "    'W_x': np.zeros_like(params['W_x']),\n",
    "    'W_h_pre': np.zeros_like(params['W_h_pre']),\n",
    "    'b_h_pre': np.zeros_like(params['b_h_pre']),\n",
    "    'W_h_cur': np.zeros_like(params['W_h_cur']),\n",
    "    'b_h_cur': np.zeros_like(params['b_h_cur']),\n",
    "})\n",
    "\n",
    "data = list(map(label_encoder, text.split()))\n",
    "\n",
    "for iter_cnt in range(max_iter):\n",
    "    cur_loss = 0\n",
    "    idx = 0\n",
    "    h_pre = np.zeros((1, unit_h))\n",
    "\n",
    "    while idx < len(data)-1:\n",
    "        X = data[idx:idx+t_size]\n",
    "        Y = data[idx+1:idx+t_size+1]\n",
    "\n",
    "        if idx+t_size+1 > len(data):\n",
    "            break\n",
    "\n",
    "        loss, cache = forward_prop(X, Y, h_pre)\n",
    "        h_pre = cache['h_s'][-2]\n",
    "        grads = backward_prop(cache, X, Y)\n",
    "\n",
    "        for param in params.keys():\n",
    "            R_AdaGrad[param] += grads[param]**2\n",
    "            params[param] -= lr*grads[param]/np.sqrt(R_AdaGrad[param]+1e-8)\n",
    "\n",
    "        idx += t_size\n",
    "        cur_loss = loss\n",
    "\n",
    "    if (iter_cnt+1) % 10 == 0:\n",
    "        print('{} iter: loss:{}'.format(iter_cnt+1, cur_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用参数预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b c d e f g h i j k\n"
     ]
    }
   ],
   "source": [
    "def predict(ch, t_size):\n",
    "    one_hot_vec = np.zeros((1, unit_O))\n",
    "    one_hot_vec[0, item2ind[ch]] = 1\n",
    "    h = np.zeros((1, unit_h))\n",
    "    res = list()\n",
    "\n",
    "    for t in range(t_size):\n",
    "        h = np.tanh(np.dot(one_hot_vec, params['W_x'])\n",
    "                    + np.dot(h, params['W_h_pre'])\n",
    "                    + params['b_h_pre'])    # 隐层状态\n",
    "        logit = softmax_row(np.dot(h, params['W_h_cur'])\n",
    "                            + params['b_h_cur'])    # softmax输出\n",
    "\n",
    "        next_item = np.random.choice(\n",
    "            range(unit_O), p=logit.ravel())    # 按概率选择下一个出现的对象\n",
    "        res.append(ind2item[next_item])\n",
    "\n",
    "        one_hot_vec = np.zeros((1, unit_O))\n",
    "        one_hot_vec[0, next_item] = 1\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "print(' '.join(predict('a', 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
