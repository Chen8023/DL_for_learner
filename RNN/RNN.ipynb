{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = \"abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz\"\n",
    "chs_set = sorted(set(data))\n",
    "ch2int = {ch: idx for idx, ch in enumerate(chs_set)}\n",
    "int2ch = {idx: ch for idx, ch in enumerate(chs_set)}\n",
    "\n",
    "def label_encoder(ch):\n",
    "    return ch2int[ch]\n",
    "\n",
    "data=list(map(label_encoder,data))    # 数字化\n",
    "\n",
    "OUT_SIZE=len(chs_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h_{t}=tanh(W_{x}x_{t}+W_{h_{t-1}}h_{t-1}+b_{h_{t-1}}) \\\\\n",
    "y_{t}=W_{h_{t}}h_{t}+b_{h_{t}} \\\\\n",
    "$$\n",
    "\n",
    "保持与之前DNN实现上的一致，易得三个权重矩阵的维度分别为：\n",
    "\n",
    "$$\n",
    "W_{x}=(x,h) \\\\\n",
    "W_{h_{t-1}}=(h,h) \\\\\n",
    "W_{h_{t}}=(h,y) \\\\\n",
    "$$\n",
    "\n",
    "参数设置与初始化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "unit_I = len(chs_set)\n",
    "unit_h = 100\n",
    "unit_O = unit_I\n",
    "\n",
    "t_size = 10    # 状态数，即时间窗口大小\n",
    "max_iter = 1000\n",
    "lr = 0.1\n",
    "\n",
    "params = OrderedDict({\n",
    "    'W_x':np.random.randn(unit_I, unit_h)*0.01,\n",
    "    'W_h_pre':np.random.randn(unit_h, unit_h)*0.01,\n",
    "    'b_h_pre':np.zeros((1, unit_h)),\n",
    "    'W_h_cur':np.random.randn(unit_h, unit_O)*0.01,\n",
    "    'b_h_cur':np.zeros((1, unit_O)),\n",
    "})\n",
    "\n",
    "h_pre = np.zeros((1, unit_h))    # h_{t-1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先是前向传播，因为RNN的一个样本有多个有序状态，所以一个样本的loss是该样本所有状态loss的累加："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_row(row_data):\n",
    "    return np.exp(row_data)/np.sum(np.exp(row_data))\n",
    "\n",
    "\n",
    "def forward_prop(x, y, h_pre):\n",
    "    '''\n",
    "    x: 数字表示的字符串\n",
    "    y: 数字表示的字符串，由x右移一位得到\n",
    "    h_pre: 隐层的上一个状态\n",
    "    '''\n",
    "    x_s = np.zeros((t_size, unit_I))    # 使用矩阵保存x当前轮的所有状态，一行为一个状态下的one-hot向量\n",
    "    h_s = np.zeros((t_size+1, unit_h))    # 使用最后一行保存上一轮的隐层状态，使用-1调用\n",
    "    h_s[-1] = h_pre\n",
    "    y_pred_s = np.zeros((t_size, unit_O))    # score\n",
    "    logit_s = np.zeros((t_size, unit_O))    # 下一个字符的概率向量\n",
    "    loss = 0\n",
    "\n",
    "    for t in range(t_size):    # 串行计算所有状态下的变量\n",
    "        x_s[t][x[t]] = 1    # one-hot向量的对应位置置1\n",
    "        h_s[t] = np.tanh(np.dot(x_s[t], params['W_x'])+np.dot(h_s[t-1], params['W_h_pre'])+params['b_h_pre'])    # 隐层状态\n",
    "        logit_s[t] = softmax_row(\n",
    "            np.dot(h_s[t], params['W_h_cur'])+params['b_h_cur'])    # softmax输出\n",
    "        loss += -np.log(logit_s[t][y[t]])    # 概率向量中，真实标签idx对应位置的值即为损失\n",
    "\n",
    "    cache={\n",
    "        'logit_s':logit_s,\n",
    "        'h_s':h_s,\n",
    "        'x_s':x_s\n",
    "    }\n",
    "    return loss, cache\n",
    "\n",
    "\n",
    "# test\n",
    "# x = data[:t_size]\n",
    "# y = data[1:t_size+1]\n",
    "# h_pre = np.zeros((1, unit_h))\n",
    "# loss, cache=forward_prop(x, y, h_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(cache, x, y):\n",
    "    logit_s, h_s, x_s = cache['logit_s'], cache['h_s'], cache['x_s']\n",
    "\n",
    "    # loss关于参数的梯度\n",
    "    grads = OrderedDict({\n",
    "        'W_x': np.zeros_like(params['W_x']),\n",
    "        'W_h_pre': np.zeros_like(params['W_h_pre']),\n",
    "        'b_h_pre': np.zeros_like(params['b_h_pre']),\n",
    "        'W_h_cur': np.zeros_like(params['W_h_cur']),\n",
    "        'b_h_cur': np.zeros_like(params['b_h_cur']),\n",
    "    })\n",
    "\n",
    "    dh_post = np.zeros_like(h_s[0])    # loss对h_{t+1}的梯度\n",
    "\n",
    "    for t in range(t_size-1, -1, -1):    # 倒推\n",
    "        dlogit = np.copy(logit_s[t])\n",
    "        dlogit[y[t]] -= 1\n",
    "\n",
    "        grads['W_h_cur'] += np.dot(np.mat(h_s[t]).T, np.mat(dlogit))    # 向量转矩阵\n",
    "        grads['b_h_cur'] += dlogit\n",
    "\n",
    "        # 后一状态的dh_post是对当前层dh_cur有贡献的\n",
    "        dh_cur = np.dot(dlogit, params['W_h_cur'].T) + dh_post\n",
    "        dh_pre = (1-h_s[t]*h_s[t])*dh_cur\n",
    "\n",
    "        grads['b_h_pre'] += dh_pre\n",
    "        grads['W_h_pre'] += np.dot(h_s[t-1].T, dh_pre)\n",
    "        grads['W_x'] += np.dot(np.mat(x_s[t]).T, np.mat(dh_pre))\n",
    "\n",
    "        dh_post = np.dot(dh_pre, params['W_h_pre'].T)    # 更新后一状态的dh_post\n",
    "\n",
    "    # 梯度截断\n",
    "    for grad in grads.values():\n",
    "        np.clip(grad, -5, 5, out=grad)\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "# test\n",
    "# grads = backward_prop(cache, x, y)\n",
    "# for param in params.keys():\n",
    "#     assert grads[param].shape==params[param].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 iter: loss:35.98180993952694\n",
      "200 iter: loss:1.037309804680368\n",
      "300 iter: loss:0.7041263367788266\n",
      "400 iter: loss:0.1770009519520916\n",
      "500 iter: loss:0.23905165689674382\n",
      "600 iter: loss:0.12560925425913708\n",
      "700 iter: loss:0.0684359747189963\n",
      "800 iter: loss:0.07240104857925224\n",
      "900 iter: loss:0.0444685356632706\n",
      "1000 iter: loss:0.06587915855459452\n"
     ]
    }
   ],
   "source": [
    "idx = 0    # 子串起始游标\n",
    "\n",
    "# AdaGrad的累计平方梯度\n",
    "R_AdaGrad = OrderedDict({\n",
    "    'W_x': np.zeros_like(params['W_x']),\n",
    "    'W_h_pre': np.zeros_like(params['W_h_pre']),\n",
    "    'b_h_pre': np.zeros_like(params['b_h_pre']),\n",
    "    'W_h_cur': np.zeros_like(params['W_h_cur']),\n",
    "    'b_h_cur': np.zeros_like(params['b_h_cur']),\n",
    "})\n",
    "\n",
    "for iter_cnt in range(max_iter):\n",
    "    if idx+t_size >= len(data) or iter_cnt == 0:\n",
    "        h_pre = np.zeros((1, unit_h))\n",
    "        idx = 0\n",
    "\n",
    "    X = data[idx:idx+t_size]\n",
    "    Y = data[idx+1:idx+t_size+1]\n",
    "\n",
    "    loss, cache = forward_prop(X, Y, h_pre)\n",
    "    h_pre = cache['h_s'][-2]\n",
    "    grads = backward_prop(cache, X, Y)\n",
    "\n",
    "    for param in params.keys():\n",
    "        R_AdaGrad[param] += grads[param]**2\n",
    "        params[param] -= lr*grads[param]/np.sqrt(R_AdaGrad[param]+1e-8)\n",
    "\n",
    "    idx += t_size\n",
    "    \n",
    "    if (iter_cnt+1)%100==0:\n",
    "        print('{} iter: loss:{}'.format(iter_cnt+1,loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用参数预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['z', ' ', 'a', 'b', 'c']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(ch, t_size):\n",
    "    one_hot_vec = np.zeros((1, unit_O))\n",
    "    one_hot_vec[0,ch2int[ch]] = 1\n",
    "    h = np.zeros((1, unit_h))\n",
    "    res=list()\n",
    "\n",
    "    for t in range(t_size):\n",
    "        h = np.tanh(np.dot(one_hot_vec, params['W_x'])\n",
    "                    + np.dot(h, params['W_h_pre'])\n",
    "                    + params['b_h_pre'])    # 隐层状态\n",
    "        logit = softmax_row(np.dot(h, params['W_h_cur'])\n",
    "                            + params['b_h_cur'])    # softmax输出\n",
    "        \n",
    "        next_item=np.random.choice(range(unit_O), p=logit.ravel())    # 按概率选择下一个出现的对象\n",
    "        res.append(int2ch[next_item])\n",
    "        \n",
    "        one_hot_vec = np.zeros((1, unit_O))\n",
    "        one_hot_vec[0,next_item] = 1\n",
    "    \n",
    "    return res\n",
    "\n",
    "predict('y',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
